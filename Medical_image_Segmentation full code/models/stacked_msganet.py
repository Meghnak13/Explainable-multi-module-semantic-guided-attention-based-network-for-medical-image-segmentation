# -*- coding: utf-8 -*-
"""stacked_MSGANet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u8f0suBYlMrKcW4IIe2GlXxeFinebJNo
"""

from functools import reduce

import pdb

import torch
import torch.nn.functional as F
import torch.nn as nn
from torch.autograd import Variable
from .attention import (
    LAM_Module,
    CWA_Module,
    EA_Module,
    semanticModule,
    LAM_CWA_Layer,
    EA_LAM_CWA_Layer,
    MultiConv
)

from .resnet101_base import ResNet101

class DAF_stack(nn.Module):
    def __init__(self):
        super(DAF_stack, self).__init__()
        self.resnet = ResNet101()

        self.down4 = nn.Sequential(
            nn.Conv2d(2048, 64, kernel_size=1), nn.BatchNorm2d(64), nn.PReLU()
        )
        self.down3 = nn.Sequential(
            nn.Conv2d(1024, 64, kernel_size=1), nn.BatchNorm2d(64), nn.PReLU()
        )
        self.down2 = nn.Sequential(
            nn.Conv2d(512, 64, kernel_size=1), nn.BatchNorm2d(64), nn.PReLU()
        )
        self.down1 = nn.Sequential(
            nn.Conv2d(256, 64, kernel_size=1), nn.BatchNorm2d(64), nn.PReLU()
        )

        inter_channels = 64
        out_channels=64

        self.conv6_1 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(64, out_channels, 1))
        self.conv6_2 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(64, out_channels, 1))
        self.conv6_3 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(64, out_channels, 1))
        self.conv6_4 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(64, out_channels, 1))

        self.conv7_1 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(64, out_channels, 1))
        self.conv7_2 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(64, out_channels, 1))
        self.conv7_3 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(64, out_channels, 1))
        self.conv7_4 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(64, out_channels, 1))

        self.conv8_1=nn.Conv2d(64,64,1)
        self.conv8_2=nn.Conv2d(64,64,1)
        self.conv8_3=nn.Conv2d(64,64,1)
        self.conv8_4=nn.Conv2d(64,64,1)
        self.conv8_11=nn.Conv2d(64,64,1)
        self.conv8_12=nn.Conv2d(64,64,1)
        self.conv8_13=nn.Conv2d(64,64,1)
        self.conv8_14=nn.Conv2d(64,64,1)

        self.softmax_1 = nn.Softmax(dim=-1)

        self.lam_attention_1_1= LAM_CWA_Layer(64, True)
        self.cwa_attention_1_1= LAM_CWA_Layer(64, False)
        self.ea_attention_1_1= EA_LAM_CWA_Layer(64, True)
         self.ea_attention_1_1= EA_LAM_CWA_Layer(64, False)
        self.semanticModule_1_1 = semanticModule(128)
        
        self.conv_sem_1_1 = nn.Conv2d(128, 64, kernel_size=3, padding=1)
        self.conv_sem_1_2 = nn.Conv2d(128, 64, kernel_size=3, padding=1)
        self.conv_sem_1_3 = nn.Conv2d(128, 64, kernel_size=3, padding=1)
        self.conv_sem_1_4 = nn.Conv2d(128, 64, kernel_size=3, padding=1)
    
    #trivial Attention mechanism
        self.lam_attention_1_2 = LAM_CWA_Layer(64)
        self.cwa_attention_1_2 = LAM_CWA_Layer(64, False)
        self.lam_attention_1_3 = LAM_CWA_Layer(64)
        self.cwa_attention_1_3 = LAM_CWA_Layer(64, False)
        self.lam_attention_1_4 = LAM_CWA_Layer(64)
        self.cwa_attention_1_4 = LAM_CWA_Layer(64, False)
        self.ea_attention_1_2 = EA_LAM_CWA_Layer(64)
        self.lam_cwa_layer_1_2 = EA_LAM_CWA_Layer(64, False)
        self.ea_attention_1_3 = EA_LAM_CWA_Layer(64)
        self.lam_cwa_layer_1_3 = EA_LAM_CWA_Layer(64, False)
        self.ea_attention_1_4 = EA_LAM_CWA_Layer(64)
        self.lam_cwa_layer_1_4 = EA_LAM_CWA_Layer(64, False)
        
        self.lam_attention_2_1 = LAM_CWA_Layer(64)
        self.cwa_attention_2_1 = LAM_CWA_Layer(64, False)
        self.ea_attention_2_1 = EA_LAM_CWA_Layer(64)
        self.lam_cwa_layer_2_1 = EA_LAM_CWA_Layer(64, False)
        self.semanticModule_2_1 = semanticModule(128)
        
        
        self.conv_sem_2_1 = nn.Conv2d(128, 64, kernel_size=3, padding=1)
        self.conv_sem_2_2 = nn.Conv2d(128, 64, kernel_size=3, padding=1)
        self.conv_sem_2_3 = nn.Conv2d(128, 64, kernel_size=3, padding=1)
        self.conv_sem_2_4 = nn.Conv2d(128, 64, kernel_size=3, padding=1)

        self.lam_attention_2_2 = LAM_CWA_Layer(64)
        self.cwa_attention_2_2 = LAM_CWA_Layer(64, False)
        self.lam_attention_2_3 = LAM_CWA_Layer(64)
        self.cwa_attention_2_3 = LAM_CWA_Layer(64, False)
        self.lam_attention_2_4 = LAM_CWA_Layer(64)
        self.cwa_attention_2_4 = LAM_CWA_Layer(64, False)
        self.ea_attention_2_2 = EA_LAM_CWA_Layer(64)
        self.lam_cwa_layer_2_2 = EA_LAM_CWA_Layer(64, False)
        self.ea_attention_2_3 = EA_LAM_CWA_Layer(64)
        self.lam_cwa_layer_2_3 = EA_LAM_CWA_Layer(64, False)
        self.ea_attention_2_4 = EA_LAM_CWA_Layer(64)
        self.lam_cwa_layer_2_4 = EA_LAM_CWA_Layer(64, False)
        
        self.fuse1 = MultiConv(256, 64, False)

        self.attention4 = MultiConv(128, 64)
        self.attention3 = MultiConv(128, 64)
        self.attention2 = MultiConv(128, 64)
        self.attention1 = MultiConv(128, 64)

        self.finetune4 = MultiConv(128, 64, False)
        self.finetune3 = MultiConv(128, 64, False)
        self.finetune2 = MultiConv(128, 64, False)
        self.finetune1 = MultiConv(128, 64, False)

        self.predict4 = nn.Conv2d(64, 5, kernel_size=1)
        self.predict3 = nn.Conv2d(64, 5, kernel_size=1)
        self.predict2 = nn.Conv2d(64, 5, kernel_size=1)
        self.predict1 = nn.Conv2d(64, 5, kernel_size=1)

        self.predict4_2 = nn.Conv2d(64, 5, kernel_size=1)
        self.predict3_2 = nn.Conv2d(64, 5, kernel_size=1)
        self.predict2_2 = nn.Conv2d(64, 5, kernel_size=1)
        self.predict1_2 = nn.Conv2d(64, 5, kernel_size=1)



    def forward(self, x):
        layer0 = self.resnet.layer0(x)
        layer1 = self.resnet.layer1(layer0)
        layer2 = self.resnet.layer2(layer1)
        layer3 = self.resnet.layer3(layer2)
        layer4 = self.resnet.layer4(layer3)

        down4 = F.upsample(self.down4(layer4), size=layer1.size()[2:], mode='bilinear')
        down3 = F.upsample(self.down3(layer3), size=layer1.size()[2:], mode='bilinear')
        down2 = F.upsample(self.down2(layer2), size=layer1.size()[2:], mode='bilinear')
        down1 = self.down1(layer1)

        predict4 = self.predict4(down4)
        predict3 = self.predict3(down3)
        predict2 = self.predict2(down2)
        predict1 = self.predict1(down1)

        fuse1 = self.fuse1(torch.cat((down4, down3, down2, down1), 1))

        semVector_1_1,semanticModule_1_1 = self.semanticModule_1_1(torch.cat((down4, fuse1),1))


        attn_lam4 = self.lam_attention_1_4(torch.cat((down4, fuse1), 1))
        attn_cwa4 = self.cwa_attention_1_4(torch.cat((down4, fuse1), 1))
        attn_ea4 = self.ea_attention_1_4(torch.cat((down4, fuse1), 1))

        attention1_4=self.conv8_1(((attn_cwa4+attn_lam4)+attn_ea4)*self.conv_sem_1_1(semanticModule_1_1))

        semVector_1_2, semanticModule_1_2 = self.semanticModule_1_1(torch.cat((down3, fuse1), 1))
        attn_lam3 = self.lam_attention_1_3(torch.cat((down3, fuse1), 1))
        attn_cwa3 = self.cwa_attention_1_3(torch.cat((down3, fuse1), 1))
        attn_ea3 = self.ea_attention_1_3(torch.cat((down3, fuse1), 1))
        attention1_3=self.conv8_2(((attn_cwa3+attn_lam3)+attn_ea3)*self.conv_sem_1_2(semanticModule_1_2))

        semVector_1_3, semanticModule_1_3 = self.semanticModule_1_1(torch.cat((down2, fuse1), 1))
        attn_lam2 = self.lam_attention_1_2(torch.cat((down2, fuse1), 1))
        attn_cwa2 = self.cwa_attention_1_2(torch.cat((down2, fuse1), 1))
        attn_ea2 = self.ea_attention_1_2(torch.cat((down3, fuse1), 1))
        attention1_2=self.conv8_3(((attn_cwa2+attn_lam2)+attn_ea2)*self.conv_sem_1_3(semanticModule_1_3))

        semVector_1_4, semanticModule_1_4 = self.semanticModule_1_1(torch.cat((down1, fuse1), 1))
        attn_lam1 = self.lam_attention_1_1(torch.cat((down1, fuse1), 1))
        attn_cwa1 = self.cwa_attention_1_1(torch.cat((down1, fuse1), 1))
        attn_ea1 = self.ea_attention_1_1(torch.cat((down3, fuse1), 1))
        attention1_1 = self.conv8_4(((attn_cwa1+attn_lam1)+attn_ea1) * self.conv_sem_1_4(semanticModule_1_4))
        
        ##new design with stacked attention

        semVector_2_1, semanticModule_2_1 = self.semanticModule_2_1(torch.cat((down4, attention1_4 * fuse1), 1))

        finetune4_1 = self.lam_attention_2_4(torch.cat((down4,attention1_4*fuse1),1))
        finetune4_2 = self.cwa_attention_2_4(torch.cat((down4,attention1_4*fuse1),1))
        finetune4_2 = self.ea_attention_2_4(torch.cat((down4,attention1_4*fuse1),1))
        finetune4 = self.conv8_11((finetune4_1+finetune4_2) * self.conv_sem_2_1(semanticModule_2_1))

        semVector_2_2, semanticModule_2_2 = self.semanticModule_2_1(torch.cat((down3, attention1_3 * fuse1), 1))
        finetune3_1 = self.lam_attention_2_3(torch.cat((down3,attention1_3*fuse1),1))
        finetune3_2 = self.cwa_attention_2_3(torch.cat((down3,attention1_3*fuse1),1))
        finetune3_2 = self.ea_attention_2_3(torch.cat((down4,attention1_4*fuse1),1))
        finetune3 = self.conv8_12((finetune3_1+finetune3_2) * self.conv_sem_2_2(semanticModule_2_2))

        semVector_2_3, semanticModule_2_3 = self.semanticModule_2_1(torch.cat((down2, attention1_2 * fuse1), 1))
        finetune2_1 = self.lam_attention_2_2(torch.cat((down2,attention1_2*fuse1),1))
        finetune2_2 = self.cwa_attention_2_2(torch.cat((down2,attention1_2*fuse1),1))
        finetune2_2 = self.ea_attention_2_2(torch.cat((down4,attention1_4*fuse1),1))
        finetune2 = self.conv8_13((finetune2_1+finetune2_2)*self.conv_sem_2_3(semanticModule_2_3))

        semVector_2_4, semanticModule_2_4 = self.semanticModule_2_1(torch.cat((down1, attention1_1 * fuse1), 1))
        finetune1_1 = self.lam_attention_2_1(torch.cat((down1,attention1_1 * fuse1),1))
        finetune1_2 = self.cwa_attention_2_1(torch.cat((down1,attention1_1 * fuse1),1))
        finetune1_2 = self.ea_attention_2_1(torch.cat((down4,attention1_4*fuse1),1))

        finetune1=self.conv8_14((finetune1_1+finetune1_2) * self.conv_sem_2_4(semanticModule_2_4))
        
        predict4_2 = self.predict4_2(finetune4)
        predict3_2 = self.predict3_2(finetune3)
        predict2_2 = self.predict2_2(finetune2)
        predict1_2 = self.predict1_2(finetune1)

        predict1 = F.upsample(predict1, size=x.size()[2:], mode='bilinear')
        predict2 = F.upsample(predict2, size=x.size()[2:], mode='bilinear')
        predict3 = F.upsample(predict3, size=x.size()[2:], mode='bilinear')
        predict4 = F.upsample(predict4, size=x.size()[2:], mode='bilinear')

        predict1_2 = F.upsample(predict1_2, size=x.size()[2:], mode='bilinear')
        predict2_2 = F.upsample(predict2_2, size=x.size()[2:], mode='bilinear')
        predict3_2 = F.upsample(predict3_2, size=x.size()[2:], mode='bilinear')
        predict4_2 = F.upsample(predict4_2, size=x.size()[2:], mode='bilinear')
        
        if self.training:
            return semVector_1_1,\
                   semVector_2_1, \
                   semVector_1_2, \
                   semVector_2_2, \
                   semVector_1_3, \
                   semVector_2_3, \
                   semVector_1_4, \
                   semVector_2_4, \
                   torch.cat((down1, fuse1), 1),\
                   torch.cat((down2, fuse1), 1),\
                   torch.cat((down3, fuse1), 1),\
                   torch.cat((down4, fuse1), 1), \
                   torch.cat((down1, attention1_1 * fuse1), 1), \
                   torch.cat((down2, attention1_2 * fuse1), 1), \
                   torch.cat((down3, attention1_3 * fuse1), 1), \
                   torch.cat((down4, attention1_4 * fuse1), 1), \
                   semanticModule_1_4, \
                   semanticModule_1_3, \
                   semanticModule_1_2, \
                   semanticModule_1_1, \
                   semanticModule_2_4, \
                   semanticModule_2_3, \
                   semanticModule_2_2, \
                   semanticModule_2_1, \
                   predict1, \
                   predict2, \
                   predict3, \
                   predict4, \
                   predict1_2, \
                   predict2_2, \
                   predict3_2, \
                   predict4_2
        else:
            return ((predict1_2 + predict2_2 + predict3_2 + predict4_2) / 4)