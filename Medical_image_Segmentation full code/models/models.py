# -*- coding: utf-8 -*-
"""models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LVElDEdZM8nUlvtjRtfmOFPtyFRs3C0B
"""

import math
import numpy as np

import torch
import torch.nn as nn
from torch.nn import functional as F
from torch.autograd import Variable
import pdb

#torch_ver = torch.__version__[:3]

__all__ = ['LAM_Module', 'CWA_Module', 'EA_Module', 'semanticModule']



class _EncoderBlock(nn.Module):
    """
    Encoder block for Semantic Attention Module
    """
    def __init__(self, in_channels, out_channels, dropout=False):
        super(_EncoderBlock, self).__init__()
        layers = [
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        ]
        if dropout:
            layers.append(nn.Dropout())
        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))
        self.encode = nn.Sequential(*layers)

    def forward(self, x):
        return self.encode(x)


class _DecoderBlock(nn.Module):
    """
    Decoder Block for Semantic Attention Module
    """
    def __init__(self, in_channels, middle_channels, out_channels):
        super(_DecoderBlock, self).__init__()
        self.decode = nn.Sequential(
            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(middle_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(middle_channels, middle_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(middle_channels),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=2, stride=2),
        )

    def forward(self, x):
        return self.decode(x)


class semanticModule(nn.Module):
    """
    Semantic attention module
    """
    def __init__(self, in_dim):
        super(semanticModule, self).__init__()
        self.chanel_in = in_dim

        self.enc1 = _EncoderBlock(in_dim, in_dim*2)
        self.enc2 = _EncoderBlock(in_dim*2, in_dim*4)
        self.dec2 = _DecoderBlock(in_dim * 4, in_dim * 2, in_dim * 2)
        self.dec1 = _DecoderBlock(in_dim * 2, in_dim, in_dim )

    def forward(self,x):

        enc1 = self.enc1(x)
        enc2 = self.enc2(enc1)

        dec2 = self.dec2( enc2)
        dec1 = self.dec1( F.upsample(dec2, enc1.size()[2:], mode='bilinear'))

        return enc2.view(-1), dec1

class LAM_Module(nn.Module):
    """ Position attention module"""
    #Ref from SAGAN
    def __init__(self, in_dim):
        super(LAM_Module, self).__init__()
        self.chanel_in = in_dim

        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)
        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)
        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)
        self.gamma = nn.Parameter(torch.zeros(1))

        self.softmax = nn.Softmax(dim=-1)
    def forward(self, x):
        """
        Parameters:
        ----------
            inputs :
                x : input feature maps( B X C X H X W)
            returns :
                out : attention value + input feature
                attention: B X (HxW) X (HxW)
        """
        m_batchsize, C, height, width = x.size()
        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)
        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)

        energy = torch.bmm(proj_query, proj_key)
        attention = self.softmax(energy)
        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)

        out = torch.bmm(proj_value, attention.permute(0, 2, 1))
        out = out.view(m_batchsize, C, height, width)

        out = self.gamma * out + x
        return out


class CWA_Module(nn.Module):
    """ Channel attention module"""
    def __init__(self, in_dim):
        super(CWA_Module, self).__init__()
        self.chanel_in = in_dim
        
        self.gamma = nn.Parameter(torch.zeros(1))
        self.softmax  = nn.Softmax(dim=-1)
    def forward(self,x):
        """
        Parameters:
        ----------
            inputs :
                x : input feature maps( B X C X H X W)
            returns :
                out : attention value + input feature
                attention: B X C X C
        """
        m_batchsize, C, height, width = x.size()
        proj_query = x.view(m_batchsize, C, -1)
        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)
       
        energy = torch.bmm(proj_query, proj_key)
        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy
        attention = self.softmax(energy_new)
        proj_value = x.view(m_batchsize, C, -1)

        out = torch.bmm(attention, proj_value)
        out = out.view(m_batchsize, C, height, width)

        out = self.gamma * out + x
        return out

class EANet(nn.Module):
    def __init__(self, n_channels, n_classes, deep_supervision=False,num_filters=64, dropout=False, rate=0.1, bn=False):
        super(EANet, self).__init__()
        self.deep_supervision = deep_supervision
        self.vgg_features = torchvision.models.vgg19(pretrained=True).features
        self.vgg_features[0]=nn.Conv2d(n_channels, 64, kernel_size=3, padding=1)
        
        self.relu = nn.ReLU(inplace=True)
        self.sigmoid = nn.Sigmoid()
        self.inc = self.vgg_features[:4]
        # ##########
        # self.vgg_features[0]=nn.Conv2d(1, 64, kernel_size=3, padding=1)
        # #######
        # self.inc1= self.vgg_features[:4]
        self.down1 = self.vgg_features[4:9]
        self.down2 = self.vgg_features[9:18]
        self.down3 = self.vgg_features[18:27]
        self.down4 = self.vgg_features[27:36]

        self.wassp=DSC(512)      #lastest ----  DSC module
        self.up1 = up(1024, 256)
        self.up2 = up(512, 128)
        self.up3 = up(256, 64)
        self.up4 = up(128, 64)
        self.outc = outconv(64+64, n_classes, dropout, rate)
        self.dsoutc4 = outconv(256+256, n_classes)
        self.dsoutc3 = outconv(128+128, n_classes)
        self.dsoutc2 = outconv(64+64, n_classes)
        self.dsoutc1 = outconv(64, n_classes)
        self.dsoutc5 = outconv(512+512, n_classes)

        # self.fuout =outconv(5, n_classes)
        
        
        #boundray stream

        self.cw = nn.Conv2d(2, 1, kernel_size=1, padding=0, bias=False)   
        self.expand = nn.Sequential(nn.Conv2d(1, num_filters, kernel_size=1),
                                    Norm2d(num_filters),
                                    nn.ReLU(inplace=True))

        self.expand1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=1),
                                    Norm2d(64),
                                    nn.ReLU(inplace=True))
        self.expand2 = nn.Sequential(nn.Conv2d(1, 128, kernel_size=1),
                                    Norm2d(128),
                                    nn.ReLU(inplace=True)) 
        self.expand3 = nn.Sequential(nn.Conv2d(1, 256, kernel_size=1),
                                    Norm2d(256),
                                    nn.ReLU(inplace=True))
        self.expand4 = nn.Sequential(nn.Conv2d(1, 512, kernel_size=1),
                                    Norm2d(512),
                                    nn.ReLU(inplace=True))                                                                                   


        self.gate1 = gsc.GatedSpatialConv2d(32, 32)
        self.gate2 = gsc.GatedSpatialConv2d(16, 16)
        self.gate3 = gsc.GatedSpatialConv2d(8, 8)

        self.c3 = nn.Conv2d(128, 1, kernel_size=1)
        self.c4 = nn.Conv2d(256, 1, kernel_size=1)
        self.c5 = nn.Conv2d(512, 1, kernel_size=1)

        self.d0 = nn.Conv2d(64, 64, kernel_size=1)
        self.res1 = ResBlock(64, 64)
        self.d1 = nn.Conv2d(64, 32, kernel_size=1)
        self.res2 = ResBlock(32, 32)
        self.d2 = nn.Conv2d(32, 16, kernel_size=1)
        self.res3 = ResBlock(16, 16)
        self.d3 = nn.Conv2d(16, 8, kernel_size=1)
        self.fuse = nn.Conv2d(8, 1, kernel_size=1, padding=0, bias=False)

    def forward(self, x):
        x_size = x.size()
        x1 = self.inc(x)
        x2 = self.down1(x1)
        # print(x2.size())
        x3 = self.down2(x2)
        # print(x3.size())
        x4 = self.down3(x3)
        # print(x4.size())
        x5 = self.down4(x4)
        # print(x5.size())
        x55=self.wassp(x5)
       
  
      ### edge stream ####### -----  EAP module

        ss = F.interpolate(self.d0(x1), x_size[2:],
                            mode='bilinear', align_corners=True)
        ss = self.res1(ss)
        c3 = F.interpolate(self.c3(x2), x_size[2:],
                            mode='bilinear', align_corners=True)
        ss = self.d1(ss)
        ss1 = self.gate1(ss, c3)
        # print("***********")
        # print(ss1.shape)
        ss = self.res2(ss1)
        ss = self.d2(ss)
        c4 = F.interpolate(self.c4(x3), x_size[2:],
                            mode='bilinear', align_corners=True)
        ss2 = self.gate2(ss, c4)
        ss = self.res3(ss2)
        ss = self.d3(ss)
        c5 = F.interpolate(self.c5(x4), x_size[2:],
                            mode='bilinear', align_corners=True)
        ss3 = self.gate3(ss, c5)
        ss = self.fuse(ss3)
        ss = F.interpolate(ss, x_size[2:], mode='bilinear', align_corners=True)
        edge_out = self.sigmoid(ss)
        

class LAM_CWA_Layer(nn.Module):
    """
    Helper Function for LAM and CWA attention
    
    Parameters:
    ----------
    input:
        in_ch : input channels
        use_lam : Boolean value whether to use LAM_Module or CWA_Module
    output:
        returns the attention map
    """
    def __init__(self, in_ch, use_lam = True):
        super(LAM_CWA_Layer, self).__init__()
        
        self.attn = nn.Sequential(
            nn.Conv2d(in_ch * 2, in_ch, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_ch),
            nn.PReLU(),
            LAM_Module(in_ch) if use_lam else CWA_Module(in_ch),
			nn.Conv2d(in_ch, in_ch, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_ch),
            nn.PReLU()
        )
    
    def forward(self, x):
        return self.attn(x)
    
class LAM_CWA_Layer(nn.Module):
    """
    Helper Function for LAM_CWA_Layer and EA attention
    
    Parameters:
    ----------
    input:
        in_ch : input channels
        use_lam : Boolean value whether to use LAM_Module or CWA_Module
    output:
        returns the attention map
    """
    def __init__(self, in_ch, use_ea = True):
        super(LAM_CWA_EA_Layer, self).__init__()
        
        self.attn = nn.Sequential(
            nn.Conv2d(in_ch * 2, in_ch, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_ch),
            nn.PReLU(),
            CA_Module(in_ch) if use_ea_Module(in_ch) else lam_cwa_layer(in_ch),
			nn.Conv2d(in_ch, in_ch, kernel_size=3, padding=1),
            nn.BatchNorm2d(in_ch),
            nn.PReLU()
        )
    
    def forward(self, x):
        return self.attn(x)
        
class MultiConv(nn.Module):
    """
    Helper function for Multiple Convolutions for refining.
    
    Parameters:
    ----------
    inputs:
        in_ch : input channels
        out_ch : output channels
        attn : Boolean value whether to use Softmax or PReLU
    outputs:
        returns the refined convolution tensor
    """
    def __init__(self, in_ch, out_ch, attn = True):
        super(MultiConv, self).__init__()
        
        self.fuse_attn = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),
            nn.BatchNorm2d(64), 
            nn.PReLU(),
            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1), 
            nn.BatchNorm2d(64), 
            nn.PReLU(),
            nn.Conv2d(out_ch, out_ch, kernel_size=1), 
            nn.BatchNorm2d(64), 
            nn.Softmax2d() if attn else nn.PReLU()
        )
    
    def forward(self, x):
        return self.fuse_attn(x)